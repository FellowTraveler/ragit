use super::Index;
use crate::chunk;
use crate::error::Error;
use crate::index::{BuildInfo, FileReader, UpdateTfidf, get_file_hash};
use ragit_api::record::Record;
use ragit_fs::{
    WriteMode,
    file_name,
    write_bytes,
};
use sha3::{Digest, Sha3_256};

impl Index {
    pub async fn build(&mut self) -> Result<(), Error> {
        let mut chunks = self.load_curr_processing_chunks()?;
        self.render_build_dashboard()?;

        let prompt = self.get_prompt("summarize")?;
        let mut hasher = Sha3_256::new();
        hasher.update(prompt.as_bytes());
        let prompt_hash = hasher.finalize();
        let prompt_hash = format!("{prompt_hash:064x}");

        while let Some(doc) = self.staged_files.pop() {
            let real_path = Index::get_data_path(
                &self.root_dir,
                &doc,
            );

            let mut fd = FileReader::new(
                doc.clone(),
                real_path.clone(),
                self.build_config.clone(),
            )?;
            self.curr_processing_file = Some(doc.clone());
            let build_info = BuildInfo::new(
                fd.file_reader_key(),
                prompt_hash.clone(),
                self.api_config.model.to_human_friendly_name().to_string(),
            );
            let mut previous_summary = None;

            while fd.can_generate_chunk() {
                self.render_build_dashboard()?;
                let chunk_path = self.get_curr_processing_chunks_path()?;
                let new_chunk = fd.generate_chunk(
                    &self,
                    &prompt,
                    build_info.clone(),
                    previous_summary.clone(),
                ).await?;
                previous_summary = Some(new_chunk.summary.clone());
                let new_chunk_uid = new_chunk.uid.clone();

                // prevents adding duplicate chunks
                if let Err(Error::NoSuchChunk { .. }) = self.get_chunk_file_by_index(&new_chunk_uid) {
                    chunks.push(new_chunk);
                    self.chunk_count += 1;
                    self.chunk_files.insert(file_name(&chunk_path)?, chunks.len());

                    // TODO: It's inefficient in that it might write the same image file multiple times.
                    //       We have to run `self.add_image_description` before `chunk::save_to_file` because
                    //       the latter requires the description files generated by the former.
                    //       The good new is that it doesn't run `self.add_image_description` multiple times
                    //       on the same image.
                    for (key, bytes) in fd.images.iter() {
                        write_bytes(
                            &Index::get_image_path(&self.root_dir, key, "png"),
                            &bytes,
                            WriteMode::CreateOrTruncate,
                        )?;
                        self.add_image_description(key).await?;
                    }

                    if chunks.len() >= self.build_config.chunks_per_json {
                        chunk::save_to_file(
                            &Index::get_chunk_path(&self.root_dir, &chunk_path),
                            &chunks,
                            self.build_config.compression_threshold,
                            self.build_config.compression_level,
                            &self.root_dir,
                            UpdateTfidf::Generate,
                        )?;

                        self.create_new_chunk_file()?;
                        chunks = self.load_curr_processing_chunks()?;
                    }

                    else {
                        chunk::save_to_file(
                            &Index::get_chunk_path(&self.root_dir, &chunk_path),
                            &chunks,
                            self.build_config.compression_threshold,
                            self.build_config.compression_level,
                            &self.root_dir,
                            UpdateTfidf::Remove,
                        )?;
                    }

                    self.add_chunk_index(&new_chunk_uid, &chunk_path, true)?;
                    self.save_to_file()?;
                }
            }

            self.processed_files.insert(doc.clone(), get_file_hash(&real_path)?);
            self.curr_processing_file = None;
            self.save_to_file()?;
        }

        self.render_build_dashboard()?;
        Ok(())
    }

    // TODO: erase lines instead of the entire screen
    fn render_build_dashboard(&self) -> Result<(), Error> {
        clearscreen::clear().expect("failed to clear screen");
        println!("staged files: {}, processed files: {}", self.staged_files.len(), self.processed_files.len());
        println!("chunks: {}, chunk files: {}", self.chunk_count, self.chunk_files.len());

        if let Some(file) = &self.curr_processing_file {
            println!("curr processing file: {file}");
        }

        else {
            println!("");
        }

        println!("model: {}", self.api_config.model.to_human_friendly_name());

        let api_records = self.api_config.get_api_usage("create_chunk_from")?;
        let mut input_tokens = 0;
        let mut output_tokens = 0;
        let mut input_cost = 0;
        let mut output_cost = 0;

        for Record { input, output, input_weight, output_weight, .. } in api_records.iter() {
            input_tokens += input;
            output_tokens += output;
            input_cost += input * input_weight;
            output_cost += output * output_weight;
        }

        println!(
            "input tokens: {input_tokens} ({:.3}$), output tokens: {output_tokens} ({:.3}$)",
            input_cost as f64 / 1_000_000_000.0,
            output_cost as f64 / 1_000_000_000.0,
        );
        Ok(())
    }
}
